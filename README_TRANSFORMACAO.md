
# ğŸ“„ README - TransformaÃ§Ã£o de Dados (dbt Project)

## ğŸ“Œ Objetivo
Este projeto de transformaÃ§Ã£o tem como objetivo organizar e padronizar os dados extraÃ­dos do MySQL e da API, criando modelos analÃ­ticos prontos para consumo em dashboards e anÃ¡lises de negÃ³cio.

O pipeline foi desenvolvido utilizando o **dbt Cloud** integrado ao **Databricks SQL Warehouse**.

---

## ğŸ—‚ï¸ Estrutura de Camadas

| Camada          | DescriÃ§Ã£o                                                                 |
|-----------------|---------------------------------------------------------------------------|
| **staging**     | Modelos brutos: aplica apenas renomeaÃ§Ãµes de colunas, castings e filtragens bÃ¡sicas. Fonte principal: tabelas criadas apÃ³s extraÃ§Ã£o. |
| **intermediate**| Modelos intermediÃ¡rios: aplica joins, cÃ¡lculos intermediÃ¡rios e prÃ©-aggrega alguns dados. |
| **marts**       | Modelos finais: modelos prontos para consumo analÃ­tico, mÃ©tricas calculadas e tabelas otimizadas para dashboards. |

---

## ğŸ·ï¸ OrganizaÃ§Ã£o de Modelos

- `models/staging/` â†’ fontes brutas do MySQL e API
- `models/intermediate/` â†’ tratamentos intermediÃ¡rios
- `models/marts/` â†’ fato e dimensÃµes finais
- `models/docs/` â†’ documentaÃ§Ã£o (opcional)
- `snapshots/` â†’ nÃ£o utilizado (ou descrever se for usado)
- `tests/` â†’ testes de qualidade (unique, not_null, etc.)

---

## ğŸ› ï¸ Ordem de ExecuÃ§Ã£o (dependÃªncias):
```plaintext
stg_* (staging) â†’ int_* (intermediate) â†’ mart_* (marts)
```

Exemplo:
```
stg_customers â†’ int_customers_orders â†’ mart_customer_lifetime_value
```

---

## âœ… Boas PrÃ¡ticas Aplicadas

- **Naming Convention**: `stg_`, `int_`, `mart_`
- **Tests**: Aplicados `not_null`, `unique` em campos crÃ­ticos
- **DocumentaÃ§Ã£o**: `.yml` de descriÃ§Ã£o dos modelos e colunas
- **Materialization**: incremental para tabelas grandes; view para tabelas leves
- **Ref() Usage**: sempre utilizar `ref()` para rastreabilidade de lineage

---

## ğŸ” ObservaÃ§Ãµes

- O projeto utiliza **Databricks SQL Warehouse** como engine de transformaÃ§Ã£o.
- A orquestraÃ§Ã£o do `dbt run` ocorre apÃ³s a extraÃ§Ã£o via notebook do **Databricks**.
- Eventuais alteraÃ§Ãµes em schemas upstream exigem atualizaÃ§Ã£o nas `sources` no `staging`.