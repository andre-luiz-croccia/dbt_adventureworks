# üìÑ README - Orquestra√ß√£o Databricks + dbt Cloud

## üìå Objetivo
Este projeto tem como objetivo orquestrar um fluxo completo de dados usando o **Databricks** para a extra√ß√£o de dados (MySQL, API) e o **dbt Cloud** para a transforma√ß√£o de dados. A integra√ß√£o garante que o pipeline de transforma√ß√£o (`dbt run`,`dbt test` ou `dbt build`) s√≥ execute ap√≥s a finaliza√ß√£o da extra√ß√£o.

---

## üéØ Fluxo Resumido

1. **Databricks Job**:
   - **Task 1**: Executa notebooks de extra√ß√£o do MySQL e API.
   - **Task 2**: Ap√≥s concluir a extra√ß√£o, dispara o Job no **dbt Cloud** via API.

2. **dbt Cloud Job**:
   - Realiza o `dbt run`,`dbt build` e `dbt test` para transformar e modelar os dados extra√≠dos.

---

## üöÄ Como Funciona

### 1. Token e IDs Necess√°rios (dbt Cloud):
- **Account ID**: `70471823477460`
- **Job ID**: `70471823485733`
- **API Token**: `dbtu_...` (armazenado de forma segura no Databricks).

> Obs: O **API Token** √© gerado no **dbt Cloud** em `Settings > API Access`.

---

### 2. Notebook Trigger dbt Cloud

O segundo notebook do Databricks utiliza a biblioteca **requests** para chamar a API do dbt Cloud logo ap√≥s finalizar a extra√ß√£o.

```python
import requests

DBT_ACCOUNT_ID = "70471823477460"
DBT_JOB_ID = "70471823485733"
DBT_API_TOKEN = "seu_token_aqui"

url = f"https://re982.us1.dbt.com/api/v2/accounts/{DBT_ACCOUNT_ID}/jobs/{DBT_JOB_ID}/run/"

payload = {"cause": "Triggered from Databricks notebook"}

headers = {
    "Authorization": f"Token {DBT_API_TOKEN}",
    "Content-Type": "application/json"
}

response = requests.post(url, headers=headers, json=payload)

if response.status_code == 200:
    run_id = response.json()["data"]["id"]
    print(f"‚úÖ dbt Cloud job triggered successfully. Run ID: {run_id}")
else:
    print(f"‚ùå Failed to trigger dbt job: {response.status_code} - {response.text}")
```

---

### 3. Organiza√ß√£o do Job no Databricks

| Task                  | Descri√ß√£o                                      | Depend√™ncia                  |
|------------------------|-----------------------------------------------|------------------------------|
| `extracao_de_dados_AW` | Notebook respons√°vel por extra√ß√£o de dados.   | Nenhuma                      |
| `Dbt_project_job`      | Notebook respons√°vel por disparar dbt Cloud. | Depende da extra√ß√£o completar |

#### Exemplo visual da estrutura no Databricks:
```
extracao_de_dados_AW
        ‚Üì
Dbt_project_job
```

---

## üìù Observa√ß√µes Importantes
- O Databricks √© o **orquestrador principal** da pipeline.
- O **dbt Cloud** √© chamado apenas ap√≥s finaliza√ß√£o bem-sucedida da extra√ß√£o.
- A trigger entre Databricks ‚û°Ô∏è dbt Cloud √© realizada via **API REST** usando Python.

---

## ‚úÖ Benef√≠cios desta Abordagem
- Orquestra√ß√£o simples, sem necessidade de Airflow ou ferramentas externas.
- Fluxo controlado inteiramente no Databricks.
- F√°cil de monitorar via Jobs no Databricks e Logs do dbt Cloud.